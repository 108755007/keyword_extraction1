{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/hodo/Desktop/keyword_extraction/jieba_based/idf_POS_collect.txt ...\n",
      "Loading model from cache /tmp/jieba.u8fbda33d7c83311d819c0f096452d7fd.cache\n",
      "Loading model cost 0.998 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_load_cut_config time consuming:  3.635 seconds\n",
      "_load_kw_config time consuming:  0.660 seconds\n",
      "add_words time consuming:  0.004 seconds\n",
      "SELECT keywords FROM article_list \n",
      "        WHERE keywords!='_' AND keywords!='' AND web_id in ('ctnews','mirrormedia','upmedia','btnet','bnext','dailyview','moneyweekly','nongnong','newscts','newtalk','setn','nownews')\n",
      "fetch_all_hashtags time consuming:  24.238 seconds\n",
      "add_words time consuming:  4.920 seconds\n",
      "add_words time consuming:  0.264 seconds\n",
      "get_stopword_list time consuming:  0.001 seconds\n",
      "SELECT web_id, usertag_keyword_expired_day FROM web_id_table where usertag_keyword_enable=1\n",
      "fetch_usertag_web_id_ex_day time consuming:  0.658 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from db import MySqlHelper\n",
    "from basic import get_date_shift, get_yesterday, to_datetime, get_today, check_is_UTC0, timing, logging_channels, date_range, datetime_to_str\n",
    "from jieba_based import Composer_jieba\n",
    "from keyword_usertag_report import keyword_usertag_report, delete_expired_rows\n",
    "import jieba.analyse\n",
    "def clean_keyword_list(keyword_list, stopwords, stopwords_usertag):\n",
    "    keyword_list = Composer_jieba().clean_keyword(keyword_list, stopwords)  ## remove stopwords\n",
    "    keyword_list = Composer_jieba().clean_keyword(keyword_list, stopwords_usertag)  ## remove stopwords, only for usertag\n",
    "    keyword_list = Composer_jieba().filter_quantifier(keyword_list)  ## remove number+quantifier, ex: 5.1萬\n",
    "    keyword_list = Composer_jieba().filter_str_list(keyword_list, pattern=\"[0-9]{2}\")  ## remove 2 digit number\n",
    "    keyword_list = Composer_jieba().filter_str_list(keyword_list, pattern=\"[0-9.]*\")  ## remove floating\n",
    "    keyword_list = Composer_jieba().filter_str_list(keyword_list, pattern=\"[a-z]{1,4}|[A-Z]{2}\")  ## remove 1-4 lowercase letter and 2 Upper\n",
    "    keyword_list = [keyword for keyword in keyword_list if keyword != ''] ## remove blank\n",
    "    return keyword_list\n",
    "def fetch_browse_record_join(web_id, date, is_df=False):\n",
    "    date_start = to_datetime(date)\n",
    "    date_end = date_start - datetime.timedelta(days=-1, seconds=1)  ## pixnet, upmedia, ctnews, cmoney,\n",
    "    query = \\\n",
    "        f\"\"\"\n",
    "            SELECT \n",
    "            s.uuid,\n",
    "            t.code,\n",
    "            t.registation_id AS token,\n",
    "            t.cert_web_id,\n",
    "            s.article_id,\n",
    "            l.title,\n",
    "            l.content,\n",
    "            l.keywords\n",
    "        FROM\n",
    "            subscriber_browse_record s\n",
    "                INNER JOIN\n",
    "            article_list l ON s.article_id = l.signature                \n",
    "                AND s.web_id = '{web_id}'                \n",
    "                AND s.click_time BETWEEN '{date_start}' AND '{date_end}'\n",
    "                AND l.web_id = '{web_id}'\n",
    "                INNER JOIN         \n",
    "            token_index t ON t.uuid = s.uuid\n",
    "                AND t.invalid = 0\n",
    "                AND t.web_id = '{web_id}'            \n",
    "        \"\"\"\n",
    "    print(query)\n",
    "    data = MySqlHelper('dione').ExecuteSelect(query)\n",
    "    if is_df:\n",
    "        df = pd.DataFrame(data, columns=['web_id', 'uuid', 'token', 'cert_web_id', 'article_id', 'title', 'content', 'keywords'])\n",
    "        return df\n",
    "    else:\n",
    "        return data\n",
    "@timing\n",
    "def fetch_usertag_web_id_ex_day():\n",
    "    query = \"SELECT web_id, usertag_keyword_expired_day FROM web_id_table where usertag_keyword_enable=1\"\n",
    "    print(query)\n",
    "    data = MySqlHelper('dione').ExecuteSelect(query)\n",
    "    web_id_all = [d[0] for d in data]\n",
    "    expired_day_all = [d[1] for d in data]\n",
    "    return web_id_all, expired_day_all\n",
    "\n",
    "def main_update_subscriber_usertag(web_id, date, is_UTC0, jump2gcp, expired_day, jieba_base, stopwords, stopwords_usertag)\n",
    "    expired_date = get_date_shift(date_ref=date, days=-expired_day, to_str=True,\n",
    "                                  is_UTC0=is_UTC0)  ## set to today + 3 (yesterday+4), preserve 4 days\n",
    "    ####從不同表讀取一些相關資料#######\n",
    "    data = fetch_browse_record_join(web_id, date=date, is_df=False)\n",
    "    \n",
    "    j, data_save = 0, {}\n",
    "    #####將資料整理 並新增usertag,cut,webid\n",
    "    for i, d in enumerate(data):\n",
    "        uuid, code, token, cert_web_id, article_id, title, content, keywords = d\n",
    "        news = title + ' ' + content\n",
    "        ## pattern for removing https\n",
    "        news_clean = jieba_base.filter_str(news, pattern=\"https:\\/\\/([0-9a-zA-Z.\\/]*)\")\n",
    "        ## pattern for removing symbol, -,+~.\n",
    "        news_clean = jieba_base.filter_symbol(news_clean)\n",
    "        ##整理keywors,沒有新增\n",
    "        if (keywords == '') | (keywords == '_'):\n",
    "            keyword_list = jieba.analyse.extract_tags(news_clean, topK=80) # TF-IDF\n",
    "            keyword_list = clean_keyword_list(keyword_list, stopwords, stopwords_usertag)[:8] #取前8關鍵字\n",
    "            keywords = ','.join(keyword_list)  ## add keywords  \n",
    "            is_cut = 1\n",
    "        else:\n",
    "            keyword_list = [k.strip() for k in keywords.split(',')]\n",
    "            keyword_list = clean_keyword_list(keyword_list, stopwords, stopwords_usertag)\n",
    "            is_cut = 0\n",
    "        ##新增usertag\n",
    "        for keyword in keyword_list:\n",
    "            data_save[j] = {'web_id': web_id, 'uuid': uuid, 'code': code, 'token': token, 'cert_web_id': cert_web_id,\n",
    "                            'news': news_clean, 'keywords': keywords, 'usertag': keyword, 'article_id': article_id,\n",
    "                            'expired_date': expired_date, 'is_cut': is_cut}\n",
    "            j += 1\n",
    "        if i % 1000 == 0:\n",
    "            \n",
    "            \n",
    "        mysqlhelper\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    is_UTC0 = check_is_UTC0()      \n",
    "    jump2gcp = True\n",
    "    date = get_yesterday(is_UTC0=is_UTC0) ## compute all browsing record yesterday ad 3:10 o'clock\n",
    "    date_list = [date]\n",
    "    jieba_base =Composer_jieba()\n",
    "    all_hashtag = jieba_base.set_config()\n",
    "    stopwords = jieba_base.get_stopword_list()\n",
    "    stopwords_usertag = jieba_base.read_file('./jieba_based/stop_words_usertag.txt')\n",
    "    web_id_all, expired_day_all = fetch_usertag_web_id_ex_day()\n",
    "    for date in date_list:\n",
    "        for web_id ,expired_day in zip(web_id_all,expired_day_all):\n",
    "            df_map_save, df_freq_token = main_update_subscriber_usertag(web_id, date, is_UTC0, jump2gcp, expired_day, jieba_base, stopwords, stopwords_usertag)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    expired_date = get_date_shift(date_ref=date, days=-4, to_str=True,\n",
    "                                  is_UTC0=is_UTC0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-04-14'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expired_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 4, 10, 0, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start = to_datetime(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 4, 10, 0, 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 4, 10, 0, 0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_clean = jieba_base.filter_str(news, pattern=\"https:\\/\\/([0-9a-zA-Z.\\/]*)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def filter_str(self, text, pattern = \"[https]{4,5}:\\/\\/[0-9a-zA-Z.\\/]*\"):\n",
    "        text_clean = ''.join(re.split(pattern, text))\n",
    "        return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_clean = ''.join(re.split(\"[https]{4,5}:\\/\\/[0-9a-zA-Z.\\/]*\", 'htttp://2131313'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = \"aavvccccddddeee\"\n",
    "\n",
    "# converting the string to a set\n",
    "temp_set = set(my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
